{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model End2End_TD_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN8UvGzSXxGdf5R5qaz/mbp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepu1992/VideoClassification/blob/master/Model_End2End_TD_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2ve0iout7M-",
        "colab_type": "text"
      },
      "source": [
        "#SETUP AND CONFIGURATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCpM9gxbnPR6",
        "colab_type": "text"
      },
      "source": [
        "##TURN SECTIONS ON/OFF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HRNrMjInTI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MOUNT_GDRIVE         = True\n",
        "CHECK_GPU_INFO       = False\n",
        "DOWNLOAD_VIDEOS      = False\n",
        "DELETE_ROI_PROCESSED = False #delete videos from video folder for which roi has been processed\n",
        "FOLDER               = 'mini'\n",
        "\n",
        "#ROI EXTRACTION\n",
        "EXTRACT_ROI          = False\n",
        "DO_TEST_TRAIN_SPLIT  = False  # set True only once for a folder at time of setup (micro,mini etc..)\n",
        "\n",
        "#FEATURE EXTRACTION\n",
        "EXTRACT_FEATURES     = False\n",
        "\n",
        "#MODEL TRAINING\n",
        "TRAIN_MODEL          = True\n",
        "CONTINUE_TRAINING    = True\n",
        "\n",
        "#PREPARE SUBMISSION\n",
        "PREPARE_SUBMISSION   = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0C245j9EEwq",
        "colab_type": "text"
      },
      "source": [
        "##PARAMETER DEFINITION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwtucG99C7UR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ROI EXTRACTION\n",
        "ROI_ROT              = False \n",
        "ERASE_ROI_COLOUR     = False  \n",
        "\n",
        "\n",
        "#FEATURE EXTRACTION\n",
        "DEPTH_OF_BASE        = 18  # > 18 means use full network\n",
        "DEPTH_OF_BASE        = min(DEPTH_OF_BASE,18)\n",
        "RESIZE_SHAPE         = (100, 100, 3)\n",
        "TRAIN_STREAM_SIZE    = 175\n",
        "TEST_STREAM_SIZE     = 50\n",
        "PREPROCESS           = True #turn on keras library preprocessing\n",
        "\n",
        "#MODEL TRAINING\n",
        "MODEL_VERSION        =  \"MODEL_V72_\"\n",
        "MINI_BATCH_SIZE      =  32\n",
        "LEARNING_RATE        =  .000001\n",
        "#KEEP_POOLING_LAYERS  =  True\n",
        "WEIGHT_BY_FRAME      =  False \n",
        "CLASS_WTS            =  'auto'#\n",
        "FRAME_SUBSET_TYPE    =  'last_n' #['last_n_subset','last_n','random',\n",
        "                                        #'even_spaced', 'random_subsection']\n",
        "MIN_FRAME_COUNT      =  40 #Videos less than this threshold are not used for training.\n",
        "EPOCH_PER_SET        =  25\n",
        "VALIDATION_SIZE      =  50\n",
        "TRAIN_DOWN_SIZE      =  175\n",
        "GLOBAL_ITER          =  40 \n",
        "FIT_1_SAMPLE         =  False\n",
        "MINI_BATCH_ITERATION =  False\n",
        "LSTM_L1_REGULARIZATION    =  .001\n",
        "LSTM_L2_REGULARIZATION    =  .001\n",
        "L1_REGULARIZATION         = .00001\n",
        "L2_REGULARIZATION         = .00001\n",
        "EARLY_STOP_PATIENCE       = 7\n",
        "RESTORE_BEST_WEIGHTS      = False\n",
        "BIGGER_SAMPLE_TEST_THRES  = .5\n",
        "\n",
        "assert not(MINI_BATCH_ITERATION and FIT_1_SAMPLE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZnL-TdolZUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import sys\n",
        "#sys.getsizeof(test_features)/np.power(10,9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp7c8ihm1PxL",
        "colab_type": "text"
      },
      "source": [
        "##CHECK GPU USAGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv9lk-8E1Mbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if CHECK_GPU_INFO:\n",
        "    # memory footprint support libraries/code\n",
        "    !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "    !pip install gputil\n",
        "    !pip install psutil\n",
        "    !pip install humanize\n",
        "    import psutil\n",
        "    import humanize\n",
        "    import os\n",
        "    import GPUtil as GPU\n",
        "    GPUs = GPU.getGPUs()\n",
        "    # XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "    gpu = GPUs[0]\n",
        "    def printm():\n",
        "      process = psutil.Process(os.getpid())\n",
        "      print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "      print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "    printm() \n",
        "\n",
        "    #!cat /proc/meminfo\n",
        "    #!cat /proc/cpuinfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1vvjeghSK2A",
        "colab_type": "text"
      },
      "source": [
        "##MOUNT GOOGLE DRIVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgKKqNyGjM5R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "0d864f2a-56cb-4138-9d45-fe473b961b8e"
      },
      "source": [
        "if MOUNT_GDRIVE:\n",
        "  from google.colab import drive \n",
        "  drive.mount('/content/gdrive') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plZDWddTJkRP",
        "colab_type": "text"
      },
      "source": [
        "##DEFINE PATH VARIABLES\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynDRWGa2VcfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import mkdir\n",
        "from os.path import isdir\n",
        "\n",
        "#define paths\n",
        "PATH_PROJ = \"/content/gdrive/My Drive/Google Colab Data/StallCatcher/\"\n",
        "PATH_ROOT = PATH_PROJ + \"{}/\".format(FOLDER)\n",
        "VIDEO_FILES_PATH =  PATH_ROOT + \"VIDEO_FOLDER/\"                    #PATH TO VIDEO FILES IN GDRIVE\n",
        "\n",
        "#TRAIN_TEST_LABEL_INFO\n",
        "PATH_TRAIN_TEST_LABEL           = PATH_ROOT + \"TRAIN_TEST_METADATA/\" +  'TRAIN_TEST_SPLIT.csv'\n",
        "PATH_TRAIN_TEST_PROCESS_INFO    = PATH_ROOT + \"TRAIN_TEST_METADATA/\" +  'TRAIN_TEST_PROCESS_INFO.csv'\n",
        "\n",
        "#ROI\n",
        "roi_rot   = (\"ROI{}_rotated/\"  if ROI_ROT else \"ROI{}/\")\n",
        "roi_erase = roi_rot.format('_ERASED' if ERASE_ROI_COLOUR else '')\n",
        "ROI_PATH = PATH_ROOT + \"ROI_FOLDER/\" + roi_erase                #PATH TO ROI DATA\n",
        "if not isdir(ROI_PATH):\n",
        "  mkdir(ROI_PATH)\n",
        "  mkdir(ROI_PATH+\"train/\")\n",
        "  mkdir(ROI_PATH+\"test/\")\n",
        "  print(\"Folder\" , ROI_PATH , \"and subfolders created\")\n",
        "PATH_ROI_PROCESS_LOGGER = ROI_PATH  + 'ROI_PROCESS_LOGGER.csv'\n",
        "\n",
        "FEATURE_USED = \"TDCNN_features_depth_{}_{}_{}/\".format(DEPTH_OF_BASE,roi_erase[:-1],RESIZE_SHAPE[0])\n",
        "if PREPROCESS:\n",
        "   FEATURE_USED = FEATURE_USED[:-1] + \"_P/\"\n",
        "\n",
        "FEATURE_PATH = PATH_ROOT  + \"FEATURE_FOLDER/\" + FEATURE_USED #PATH TO FEATURES EXTRACTED FROM ROI\n",
        "#FEATURE_PATH = PATH_ROOT + \"FEATURE_FOLDER/\" + 'extracted_features_full_depth/'\n",
        "if not isdir(FEATURE_PATH):\n",
        "  mkdir(FEATURE_PATH)\n",
        "  mkdir(FEATURE_PATH+\"train/\")\n",
        "  mkdir(FEATURE_PATH+\"test/\")\n",
        "  print(\"Folder\" , FEATURE_PATH , \"created\")\n",
        "PATH_FEATURE_PROCESS_LOGGER = FEATURE_PATH  + 'FEATURE_PROCESS_LOGGER.csv'\n",
        "\n",
        "\n",
        "MODEL_FOLDER = PATH_ROOT + \"MODEL_FOLDER/\"\n",
        "UTILS_PATH = \"/content/gdrive/My Drive/Colab Notebooks/StallCatcher/\"\n",
        "LOGS_DIR = MODEL_FOLDER + \"logs/fit/\"\n",
        "\n",
        "\n",
        "#define filenames\n",
        "METADATA_FILENAME = PATH_ROOT + 'train_metadata_{}.csv'.format(FOLDER) #FILES LISTED WILL BE USED FOR TRAINING AND VALIDATION\n",
        "LABELS_FILENAME   = PATH_PROJ + 'train_labels.csv'\n",
        "\n",
        "MODEL_SUB_FODLER  = MODEL_FOLDER + MODEL_VERSION[:-1] + \"/\"\n",
        "MODEL_FILENAME    = MODEL_SUB_FODLER + MODEL_VERSION\n",
        "BEST_MODEL_FILE   = MODEL_SUB_FODLER + 'MODEL_BEST'\n",
        "\n",
        "if not isdir(MODEL_SUB_FODLER):\n",
        "  mkdir(MODEL_SUB_FODLER)\n",
        "  print(\"Folder\" , MODEL_SUB_FODLER , \"created\")\n",
        "\n",
        "MODEL_RESULTS     = MODEL_SUB_FODLER + \"MODEL_RESULTS/\"\n",
        "PATH_TRAIN_ERRORS = MODEL_RESULTS + 'ERROR_LOG.csv'\n",
        "HYP_PARAM_FILE    = MODEL_RESULTS + 'MODEL_SETTING_HYPER_PARAM.csv'\n",
        "\n",
        "if not isdir(MODEL_RESULTS):\n",
        "  mkdir(MODEL_RESULTS)\n",
        "  print(\"Folder\", MODEL_RESULTS , \"created\")\n",
        "\n",
        "for i in ['PERFORMANCE', 'MCC', 'LOSS_GRAPH', 'ACC_GRAPH', 'MCC_GRAPH','OUT_HIST']:\n",
        "      if not isdir(MODEL_RESULTS + i):\n",
        "        mkdir(MODEL_RESULTS + i)\n",
        "        print(\"Folder\", MODEL_RESULTS + i , \"created\")\n",
        "\n",
        "#RESULTS FORMATS\n",
        "PERF_FILE_FORMAT  = MODEL_RESULTS + \"PERFORMANCE/\" + \"Perf_\"\n",
        "MODEL_PERF_FILE   = PERF_FILE_FORMAT + MODEL_VERSION + \"{}.csv\"\n",
        "\n",
        "MODEL_MCC_FORMAT     = MODEL_RESULTS + \"MCC/\" + MODEL_VERSION + \"{}.csv\"\n",
        "LOSS_IMAGE_NAME      = MODEL_RESULTS + \"LOSS_GRAPH/\" + MODEL_VERSION + \"{}.jpeg\"\n",
        "ACC_IMAGE_NAME       = MODEL_RESULTS + \"ACC_GRAPH/\"  + MODEL_VERSION + \"{}.jpeg\"\n",
        "MCC_IMAGE_NAME       = MODEL_RESULTS + \"MCC_GRAPH/\"  + MODEL_VERSION + \"{}.jpeg\"\n",
        "OUT_DIST_IMAGE_NAME  = MODEL_RESULTS + \"OUT_HIST/\"   + MODEL_VERSION + \"{}.jpeg\"\n",
        "CONFUSION_MAT_PATH   = MODEL_RESULTS + \"CONFUSION_MAT.csv\"    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWGM-jyLVRvG",
        "colab_type": "text"
      },
      "source": [
        "##IMPORT LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhO96iA1VP8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re \n",
        "import h5py\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "import warnings\n",
        "from math import ceil\n",
        "import sys\n",
        "\n",
        "#warnings.filterwarnings('ignore') #u\n",
        "\n",
        "from tensorflow.keras import models,layers,optimizers,losses\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.layers import LSTM, Input, Flatten\n",
        "from tensorflow.keras.regularizers import L1L2\n",
        "from random import shuffle\n",
        "from sklearn.metrics import confusion_matrix,matthews_corrcoef\n",
        "from tqdm.notebook import tqdm\n",
        "from time import sleep\n",
        "from skimage.transform import resize   # for resizing images\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from sklearn.model_selection import train_test_split\n",
        "from random import sample,choice,randint\n",
        "\n",
        "#check if running on GPU\n",
        "if tf.test.gpu_device_name() == '/device:GPU:0':\n",
        "  print(\"Tensorflow GPU Loaded\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq9KkkFCyXe_",
        "colab_type": "text"
      },
      "source": [
        "##LOAD USER DEFINED FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePNkdGgGyUgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if False: #ignore, keeping all functions in the file\n",
        "  import sys\n",
        "  sys.path.append(UTILS_PATH)\n",
        "  from utilities import extract_roi_and_store_tensors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6JMDzd3vk0g",
        "colab_type": "text"
      },
      "source": [
        "##LOAD METADATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COKgLjcBvhjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_metadata = pd.read_csv( METADATA_FILENAME)\n",
        "list_of_files = list(train_metadata.filename)\n",
        "train_metadata.set_index('filename',inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpQgcEMcugll",
        "colab_type": "text"
      },
      "source": [
        "#DOWNLOADING AND PREPROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0xT2SD3JySd",
        "colab_type": "text"
      },
      "source": [
        "##DOWNLOAD VIDEOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBe9y2X6dF4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if DOWNLOAD_VIDEOS :\n",
        "  #import library to interact with s3\n",
        "  import boto3\n",
        "\n",
        "  #setup s3 access\n",
        "  s3r = boto3.resource('s3',\n",
        "                      aws_access_key_id='INSERT_ACCESS_KEY',\n",
        "                      aws_secret_access_key= 'INSERT_SECRET_ACCESS_KEY')\n",
        "  buck = s3r.Bucket('drivendata-competition-clog-loss')\n",
        "\n",
        "  failed = []\n",
        "  \n",
        "  already_downloaded_files = [f for f in listdir(VIDEO_FILES_PATH) if isfile(join(VIDEO_FILES_PATH, f))]\n",
        "  videos_to_download = set.difference(set(list_of_files),set(already_downloaded_files)) \n",
        "\n",
        "  for f in tqdm(videos_to_download, position=0, leave=True):\n",
        "      try:\n",
        "        s3_path_of_video = \"train/\" + f\n",
        "        buck.download_file(s3_path_of_video,VIDEO_FILES_PATH + f) \n",
        "      except:\n",
        "        print(\"Failed - \", f)\n",
        "        failed.append(f)\n",
        "        print(\".\", sep = \"\", end = \"\")\n",
        "  if len(failed) > 0:\n",
        "    print(str(len(failed)) + \" files failed to load.\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw9Brd2hGzwy",
        "colab_type": "text"
      },
      "source": [
        "## DELETE PROCESSED VIDEOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-N1GsoiE8II",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if DELETE_ROI_PROCESSED:\n",
        "  from os import remove\n",
        "  logger                 = pd.read_csv(PATH_ROI_PROCESS_LOGGER, index_col= 0)\n",
        "  processed_files        = logger[logger.ROI_PROCESSED].index\n",
        "\n",
        "  for files in tqdm(processed_files):\n",
        "      p_file = VIDEO_FILES_PATH + files\n",
        "      if isfile(p_file):\n",
        "        remove(p_file)\n",
        "        print(\"*\", end = '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo_Qe0R-6QpB",
        "colab_type": "text"
      },
      "source": [
        "##ROI FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh4HlzEPCllk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_roi(frame, viz = False):\n",
        "    \n",
        "    #get orange coloured cells as 255 and rest as 0                                                                                                                                    \n",
        "    th = cv2.inRange(frame, (0, 13, 104), (98, 143, 255))  \n",
        "\n",
        "    #return row and column indices of orange cells                                                                                             \n",
        "    points = np.where(th>0)            \n",
        "    if ERASE_ROI_COLOUR:\n",
        "      frame[points] = 0\n",
        "    #get coordinates                                                                                                                                  \n",
        "    p2 = zip(points[0], points[1])                                                                                                                                       \n",
        "    p2 = [p for p in p2]               \n",
        "\n",
        "    #get bounding box                                                                                                                                  \n",
        "    rect = cv2.boundingRect(np.float32(p2))\n",
        "    \n",
        "    #get rectangle\n",
        "    row_min,row_max,col_min,col_max = rect[0],rect[0]+rect[2],rect[1],rect[1]+rect[3]\n",
        "\n",
        "    roi = frame[row_min:row_max,col_min:col_max,:]  \n",
        "  \n",
        "    if viz:\n",
        "        cv2.imshow(\"short\", roi)                                                                                                                                                  \n",
        "        cv2.waitKey(1000)            \n",
        "        cv2.destroyAllWindows()\n",
        "    return roi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMzgnhbQ_QWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ROI_inclined_rect(frame, viz = False):\n",
        "\n",
        "    #get orange coloured cells as 255 and rest as 0                                                                                                                                    \n",
        "    th = cv2.inRange(frame, (0, 13, 104), (98, 143, 255)) \n",
        "\n",
        "    #return row and column indices of orange cells                                                                                             \n",
        "    points = np.where(th>0)\n",
        "\n",
        "    if ERASE_ROI_COLOUR:\n",
        "      frame[points] = 0\n",
        "    #get coordinates                                                                                                                                  \n",
        "    p2 = zip(points[1], points[0])                                                                                                                                       \n",
        "    p2 = [p for p in p2]        \n",
        "    \n",
        "    # find rotated rectangle\n",
        "    rect = cv2.minAreaRect(np.float32(p2))\n",
        "    \n",
        "    # rotate img\n",
        "    angle = rect[2]\n",
        "    rows, cols = frame.shape[0], frame.shape[1]\n",
        "    M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
        "    img_rot = cv2.warpAffine(frame, M, (cols, rows))\n",
        "\n",
        "    # rotate bounding box\n",
        "    #rect0 = (rect[0], rect[1], 0.0)\n",
        "    box = cv2.boxPoints(rect)\n",
        "    pts = np.int0(cv2.transform(np.array([box]), M))[0]\n",
        "    pts[pts < 0] = 0\n",
        "\n",
        "    # crop\n",
        "    img_crop = img_rot[pts[1][1]:pts[0][1],\n",
        "               pts[1][0]:pts[2][0]]\n",
        "    \n",
        "    if viz:        \n",
        "        cv2.imshow('xyz', img_crop)                                                                                                                                                  \n",
        "        cv2.waitKey(1000)\n",
        "        cv2.destroyAllWindows()              \n",
        "    return img_crop "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvoODU4H0sef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_roi_and_store_tensors(filenames_list, \n",
        "                                  sample_type,\n",
        "                                  roi_path, \n",
        "                                  video_path, \n",
        "                                  batch_size = 1000,\n",
        "                                  start_at = None,\n",
        "                                  submission = False):\n",
        "\n",
        "    print(\"Extracting ROI and loading to \", sample_type, \"\\n\")\n",
        "    sleep(1)\n",
        "\n",
        "    roi_log_path = PATH_SUB_ROI_PROCESS_LOGGER if submission else PATH_ROI_PROCESS_LOGGER\n",
        "\n",
        "    #sub function to log roi progress\n",
        "    def log_progress(batch_success_files, h5_file_count):\n",
        "              #update logger\n",
        "              logger = pd.read_csv(roi_log_path, index_col= 0)\n",
        "              logger.loc[batch_success_files,'ROI_PROCESSED'] = True\n",
        "              logger.loc[batch_success_files,'ROI_FILENAME'] = sample_type + str(h5_file_count) + '.h5'\n",
        "              logger.to_csv(roi_log_path, index = True)\n",
        "\n",
        "    #check if any files are already processed and remove them\n",
        "    logger = pd.read_csv(roi_log_path, index_col= 0)\n",
        "    if any(logger.loc[filenames_list,'ROI_PROCESSED'].to_list()):\n",
        "              sub = logger.loc[filenames_list]\n",
        "              dup = sub[sub.ROI_PROCESSED]\n",
        "              print(\"Avoiding files already processed: \", dup.index.to_list())\n",
        "              filenames_list = sub[~sub.ROI_PROCESSED].index.to_list()\n",
        "\n",
        "              if len(filenames_list) == 0:\n",
        "                print(\"All files are already processed\")\n",
        "                return\n",
        "\n",
        "    assert len(filenames_list) > 0\n",
        "    \n",
        "    print(\"Number of files to be processed: \", len(filenames_list))\n",
        "\n",
        "    #path to save roi\n",
        "    temp_path = roi_path + sample_type + \"/\" + sample_type\n",
        "    \n",
        "    #h5 file id\n",
        "    batch_success_files = []    \n",
        "    if start_at is None: #auto determine if left as none\n",
        "\n",
        "        logger   = pd.read_csv(roi_log_path, index_col= 0)\n",
        "\n",
        "        st    = logger[logger.sample_type == sample_type]\n",
        "        used  = st[st.ROI_FILENAME != 'NOT_AVAILABLE']\n",
        "\n",
        "        if used.shape[0] > 0:\n",
        "          last_file  = max([ int((re.findall('\\d+', s ))[0]) for s in used.ROI_FILENAME])\n",
        "          start_at = last_file + 1\n",
        "        else:\n",
        "          start_at = 0\n",
        "    \n",
        "    #id of the file file to be written\n",
        "    h5_file_count = start_at      \n",
        "\n",
        "    #file writer object\n",
        "    h5f   = h5py.File( temp_path + str(h5_file_count) + '.h5', 'w')    \n",
        "    ind = 0\n",
        "\n",
        "    print(\"Files Saved: \")\n",
        "    for videoFile in tqdm(filenames_list):               \n",
        "        \n",
        "\n",
        "        #initiate next batch\n",
        "        if ind%batch_size == 0 and ind != 0:\n",
        "           print(sample_type+str(h5_file_count), end = \" | \")\n",
        "           \n",
        "           h5f.close()\n",
        "\n",
        "           #log progress\n",
        "           log_progress(batch_success_files, h5_file_count)\n",
        "\n",
        "           #refresh list\n",
        "           batch_success_files =  []\n",
        "\n",
        "           h5_file_count       += 1\n",
        "           h5f                 =  h5py.File(temp_path + str(h5_file_count) + '.h5', 'w')        \n",
        "        \n",
        "        #update iteration count\n",
        "        ind += 1\n",
        "\n",
        "        #id image\n",
        "        count = 0    \n",
        "        \n",
        "        #full video path\n",
        "        video_input_path = video_path + videoFile\n",
        "        \n",
        "        # read video\n",
        "        cap = cv2.VideoCapture(video_input_path)           \n",
        "        \n",
        "        #number of frames\n",
        "        n_frame = int(cap.get(7))\n",
        "\n",
        "        #to get shape of tensor\n",
        "        do_once = True\n",
        "\n",
        "        while(cap.isOpened()):\n",
        "            #frameId = cap.get(1)\n",
        "            ret, frame = cap.read()\n",
        "            if (ret == False):\n",
        "                break\n",
        "           \n",
        "            if True: #sample frame here if required using frameId\n",
        "                \n",
        "                #subset roi from image\n",
        "                if ROI_ROT:\n",
        "                  roi = get_ROI_inclined_rect(frame)\n",
        "                else:\n",
        "                  roi = get_roi(frame)\n",
        "\n",
        "                if do_once:\n",
        "                  #placeholder to store roi frames\n",
        "                  h,w,c = roi.shape\n",
        "                  roi_tensor = np.zeros((n_frame, h,w,c)) \n",
        "                  do_once = False \n",
        "                  \n",
        "\n",
        "                if roi.shape[0] != h or roi.shape[1] != w:\n",
        "                  roi = resize(roi, (h, w), \n",
        "                           preserve_range = True,\n",
        "                           anti_aliasing=False) \n",
        "\n",
        "                #write image\n",
        "                roi_tensor[count] = roi\n",
        "\n",
        "                #update counter\n",
        "                count+=1\n",
        "\n",
        "        if not do_once:\n",
        "                #normalize and save\n",
        "                h5f.create_dataset(videoFile, data= roi_tensor)                      \n",
        "                \n",
        "                #release cap object\n",
        "                cap.release()\n",
        "\n",
        "                #append filename to success list\n",
        "                batch_success_files.append(videoFile)\n",
        "\n",
        "    print(sample_type+str(h5_file_count), end = \" | \")\n",
        "    h5f.close()      \n",
        "    log_progress(batch_success_files, h5_file_count)\n",
        "    sleep(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8Ve2TmH6GF3",
        "colab_type": "text"
      },
      "source": [
        "## TEST TRAIN SPLIT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISJVraTK5laM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if EXTRACT_ROI  and DO_TEST_TRAIN_SPLIT:\n",
        "    #get list of video files available   \n",
        "    video_filenames  = list(train_metadata.index)\n",
        "\n",
        "    #split test train\n",
        "    train,test = train_test_split(video_filenames,test_size = 0.1) #-- run only during project setup\n",
        "    \n",
        "    #mark all undecided \n",
        "    TEST_TRAIN_SPLIT                = train_metadata\n",
        "    TEST_TRAIN_SPLIT['sample_type'] = 'UNDECIDED'\n",
        "\n",
        "    TEST_TRAIN_SPLIT.loc[train,'sample_type'] = 'train'\n",
        "    TEST_TRAIN_SPLIT.loc[test ,'sample_type'] = 'test'\n",
        "\n",
        "    #save to disk\n",
        "    TEST_TRAIN_SPLIT.to_csv(PATH_TRAIN_TEST_LABEL, index = True)\n",
        "\n",
        "    #ROI PROCESS LOGGER\n",
        "    TEST_TRAIN_SPLIT['ROI_PROCESSED'] = False\n",
        "    TEST_TRAIN_SPLIT['ROI_FILENAME'] = 'NOT_AVAILABLE'\n",
        "    TEST_TRAIN_SPLIT.to_csv(PATH_ROI_PROCESS_LOGGER, index = True)\n",
        "\n",
        "    DO_TEST_TRAIN_SPLIT = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8g3IFvqNDC5",
        "colab_type": "text"
      },
      "source": [
        "##EXTRACT ROI "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5maF1rVDgsP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if EXTRACT_ROI and (not DO_TEST_TRAIN_SPLIT):\n",
        "\n",
        "    #read roi log file and remove already processed files\n",
        "    logger        = pd.read_csv(PATH_ROI_PROCESS_LOGGER, index_col= 0)\n",
        "    logger        = logger[~logger.ROI_PROCESSED]\n",
        "\n",
        "    #files yet to be processed\n",
        "    train         = logger[logger.sample_type == 'train'].index.tolist()\n",
        "    test          = logger[logger.sample_type == 'test'].index.tolist()\n",
        "         \n",
        " \n",
        "    if len(test) > 0:\n",
        "      #extract roi and save for train and test   \n",
        "      extract_roi_and_store_tensors(filenames_list = test,  \n",
        "                                      sample_type = 'test', \n",
        "                                      roi_path     = ROI_PATH, \n",
        "                                      video_path   = VIDEO_FILES_PATH,\n",
        "                                      batch_size   = TEST_STREAM_SIZE)\n",
        "    else:\n",
        "      print(\"Test file already ROI processed\")\n",
        "  \n",
        "    if len(train)> 0:\n",
        "      extract_roi_and_store_tensors(filenames_list = train, \n",
        "                                    sample_type    = 'train', \n",
        "                                    roi_path       = ROI_PATH, \n",
        "                                    video_path     = VIDEO_FILES_PATH,\n",
        "                                    batch_size     = TRAIN_STREAM_SIZE)\n",
        "    else:\n",
        "      print(\"Train file already ROI processed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf2WsQg16eAh",
        "colab_type": "text"
      },
      "source": [
        "## FEATURE EXTRACTION FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLSQFFnpM6Vw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sub function to log roi progress\n",
        "def log_feature_batch(files, h5_file_name = \"FAILED\",submission = False):\n",
        "          #\"\"\" record success or failure of a batch \"\"\"\n",
        "\n",
        "          feat_log_path= PATH_SUB_FEATURE_PROCESS_LOGGER if submission else PATH_FEATURE_PROCESS_LOGGER\n",
        "\n",
        "          #update logger\n",
        "          logger = pd.read_csv(feat_log_path, index_col = 0)  \n",
        "          logger.loc[files,'FEATURE_FILENAME']  = h5_file_name\n",
        "          logger.to_csv(feat_log_path, index    = True)\n",
        "\n",
        "#sub function to log succes/failure of roi to feature convertion\n",
        "def log_feature_sample(files, feature_processed,submission = False):\n",
        "\n",
        "          feat_log_path= PATH_SUB_FEATURE_PROCESS_LOGGER if submission else PATH_FEATURE_PROCESS_LOGGER\n",
        "\n",
        "          #update logger\n",
        "          logger = pd.read_csv(feat_log_path, index_col = 0)              \n",
        "          logger.loc[files,'FEATURE_PROCESSED']               = feature_processed              \n",
        "          logger.to_csv(feat_log_path, index    = True)\n",
        "\n",
        "\n",
        "def extract_features(roi_files,\n",
        "                     sample_type, \n",
        "                     roi_path, \n",
        "                     feature_path,\n",
        "                     flatten_features = False,\n",
        "                     resize_to_shape =  (100,100,3),\n",
        "                     batch_size = 200,\n",
        "                     start_at = None,\n",
        "                     submission = False):\n",
        "  \n",
        "    feat_log_path= PATH_SUB_FEATURE_PROCESS_LOGGER if submission else PATH_FEATURE_PROCESS_LOGGER\n",
        "\n",
        "    if start_at is None: #auto determine if left as none\n",
        "\n",
        "        logger   = pd.read_csv(feat_log_path, index_col= 0)\n",
        "\n",
        "        st    = logger[logger.sample_type == sample_type]\n",
        "        used  = st[st.FEATURE_FILENAME != 'NOT_AVAILABLE']\n",
        "\n",
        "        if used.shape[0] > 0:\n",
        "          files_used = [i for i in used.FEATURE_FILENAME.unique() if sample_type in i]\n",
        "          last_file  = max([ int((re.findall('\\d+', s ))[0]) for s in files_used])\n",
        "          start_at = last_file + 1\n",
        "         \n",
        "        else:\n",
        "          start_at = 0\n",
        "\n",
        "\n",
        "    #TRAIN/TEST folder in ROI folder\n",
        "    input_folder_path = roi_path + sample_type + \"/\"\n",
        "    \n",
        "    #TRAIN/TEST folder in FEATURE folder\n",
        "    output_folder_path = feature_path + sample_type + \"/\"\n",
        "    \n",
        "    failed_roi_files    = []\n",
        "    failed_samples      = []\n",
        "\n",
        "    print(f\"\\n\\nRunning feature extraction on {sample_type} ROI tensors\")\n",
        "    sleep(1)\n",
        "        \n",
        "    #print_format = \"Processing batch {}/\" + str(len(roi_files))\n",
        "\n",
        "    #path to save roi\n",
        "    temp_path = feature_path + sample_type + \"/\" + sample_type\n",
        "\n",
        "    #h5 file id\n",
        "    write_file_count = start_at\n",
        "    feature_write   = h5py.File( temp_path + str(write_file_count) + '.h5', 'w')\n",
        "    \n",
        "    samples_processed = 0\n",
        "\n",
        "    for roi_file_name in tqdm(roi_files):\n",
        "\n",
        "            #print(print_format.format(batch+1)) #(enumerate on roi_file for batch value)\n",
        "            if True:\n",
        "            #try:       \n",
        "                #create h5py read object for roi data\n",
        "                if 'roi_read' in locals():\n",
        "                  roi_read.close()\n",
        "                roi_read = h5py.File(input_folder_path  + roi_file_name,'r')\n",
        "            \n",
        "                #get video files in object\n",
        "                shortFiles = []\n",
        "                dirFILES   = []\n",
        "\n",
        "                files_in_obj = list(roi_read.keys())\n",
        "                logger_ = pd.read_csv(feat_log_path, index_col = 0)    \n",
        "                logger_ = logger_.loc[files_in_obj]\n",
        "                na_files = logger_[logger_.FEATURE_FILENAME == 'NOT_AVAILABLE'].index\n",
        "                del logger_\n",
        "\n",
        "                if not submission:\n",
        "                  for i in na_files:\n",
        "                    if train_metadata.loc[i, 'num_frames'] >= MIN_FRAME_COUNT:\n",
        "                      dirFILES.append(i)\n",
        "                    else:\n",
        "                      shortFiles.append(i)\n",
        "\n",
        "                  #records files discarded due to  FRAME_COUNT < MIN_FRAME_COUNT\n",
        "                  log_feature_sample(shortFiles, \n",
        "                                      feature_processed = 'INADEQUATE_FRAME_COUNT',\n",
        "                                      submission = submission) \n",
        "                else:\n",
        "                  dirFILES = files_in_obj\n",
        "                  shortFiles = []         \n",
        "                \n",
        "                \n",
        "                for file_ in dirFILES:\n",
        "\n",
        "                    #initiate next batch\n",
        "                    if samples_processed%batch_size == 0 and samples_processed != 0:\n",
        "                      \n",
        "                      #name of successfully processed feature file\n",
        "                      success_file_name = sample_type + str(write_file_count) + '.h5'\n",
        "                      print(success_file_name , end = \" | \")\n",
        "\n",
        "                      #log success of feature batch\n",
        "                      files = list(feature_write.keys())\n",
        "                      log_feature_batch(files, \n",
        "                                        h5_file_name = success_file_name,\n",
        "                                        submission = submission)\n",
        "\n",
        "                      #close writer\n",
        "                      feature_write.close()\n",
        "                      \n",
        "                      #update feature filename and open feature file writer\n",
        "                      write_file_count +=1\n",
        "                      feature_write = h5py.File(temp_path + str(write_file_count) + '.h5', 'w')  \n",
        "\n",
        "                    try:\n",
        "                        #read roi\n",
        "                        roi_tensor = roi_read[file_][:]\n",
        "\n",
        "                        if submission and roi_tensor.shape[0] < MIN_FRAME_COUNT:\n",
        "                          shortFiles.append(file_)\n",
        "\n",
        "                        else:\n",
        "                          roi_tensor = roi_tensor[-MIN_FRAME_COUNT:]\n",
        "                          #resize_value will hold the resized frames of the roi tensor\n",
        "                          resized_tensor = np.zeros((MIN_FRAME_COUNT, resize_to_shape[0],\n",
        "                                                  resize_to_shape[1],resize_to_shape[2]))\n",
        "                          \n",
        "                          for frame in range(roi_tensor.shape[0]):\n",
        "\n",
        "                              if PREPROCESS:\n",
        "                                  image_ = roi_tensor[frame]/255\n",
        "                              else:\n",
        "                                  image_ = roi_tensor[frame]\n",
        "\n",
        "                              #resize    \n",
        "                              resized_tensor[frame] = resize(image_,\n",
        "                                                  resize_to_shape, \n",
        "                                                  preserve_range = True,\n",
        "                                                  anti_aliasing=False)  \n",
        "                              \n",
        "                          #apply pre trained model on roi frame of sample            \n",
        "                          feature = np.array(resized_tensor)\n",
        "                \n",
        "                          feature_write.create_dataset(file_, data= feature)   \n",
        "                          samples_processed += 1\n",
        "                    except:\n",
        "                          failed_samples.append(file_)   \n",
        "                          print(\"-fail-\",end = \"\")\n",
        "\n",
        "                #failed files\n",
        "                log_feature_sample(failed_samples, feature_processed = \"FAILED\",\n",
        "                      submission = submission)\n",
        "\n",
        "                #successfully processed files\n",
        "                proc = [i for i in dirFILES if i not in failed_samples]\n",
        "                log_feature_sample(proc, feature_processed = \"SUCCESS\",submission = submission)\n",
        "\n",
        "\n",
        "                #close roi read object                \n",
        "                roi_read.close()\n",
        "            else:   \n",
        "            #except:\n",
        "              failed_roi_files.append(roi_file_name)\n",
        "              print(\"roi \", roi_file_name , \"failed\", end = \" | \")\n",
        "              logger = pd.read_csv(feat_log_path, index_col = 0) \n",
        "              logger.loc[logger.ROI_FILENAME == roi_file_name,'FEATURE_FILENAME']   =  'ROI_READ_FAIL'\n",
        "              logger.loc[logger.ROI_FILENAME == roi_file_name,'FEATURE_PROCESSED']  =  'ROI_READ_FAIL'\n",
        "              logger.to_csv(feat_log_path, index  = True)\n",
        "\n",
        " \n",
        "    #name of successfully processed feature file\n",
        "    success_file_name = sample_type + str(write_file_count) + '.h5'\n",
        "    print(success_file_name , end = \" | \")\n",
        "\n",
        "    #log success of feature batch\n",
        "    files = list(feature_write.keys())\n",
        "    log_feature_batch(files, \n",
        "                      h5_file_name = success_file_name,\n",
        "                      submission = submission)\n",
        "                      \n",
        "    #close last feature write file\n",
        "    feature_write.close()\n",
        "\n",
        "    return failed_samples,failed_roi_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rLzLXbqOH-p",
        "colab_type": "text"
      },
      "source": [
        "##EXTRACT FEATURES "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJb5eIu8qfpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if EXTRACT_FEATURES:\n",
        "\n",
        "    #READ FEATURE LOGGER\n",
        "    try:\n",
        "      FEATURE_PROCESS_LOGGER = pd.read_csv(PATH_FEATURE_PROCESS_LOGGER, \n",
        "                                           index_col = 0)\n",
        "      \n",
        "      #refresh ROI_PROCESSED INFORMATION\n",
        "      roi_log         = pd.read_csv(PATH_ROI_PROCESS_LOGGER, index_col= 0)\n",
        "      cols_to_refresh = ['ROI_PROCESSED','ROI_FILENAME']\n",
        "      FEATURE_PROCESS_LOGGER.drop(columns=cols_to_refresh, inplace = True)\n",
        "      FEATURE_PROCESS_LOGGER = pd.merge(FEATURE_PROCESS_LOGGER,roi_log[cols_to_refresh],\n",
        "                                        on = 'filename', \n",
        "                                        validate = 'one_to_one')\n",
        "\n",
        "\n",
        "      print(\"Loaded FEATURE_PROCESS_LOGGER\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "      print(\"No FEATURE_PROCESS_LOGGER available. Creating new one..\")\n",
        "\n",
        "      #create feature logger from ROI logger\n",
        "      FEATURE_PROCESS_LOGGER                      = pd.read_csv(PATH_ROI_PROCESS_LOGGER, index_col= 0)\n",
        "\n",
        "      FEATURE_PROCESS_LOGGER['FEATURE_PROCESSED'] = 'UNATTEMPTED'  #False refers to unattempted files\n",
        "      FEATURE_PROCESS_LOGGER['FEATURE_FILENAME']  = 'NOT_AVAILABLE'\n",
        "\n",
        "      #save to disk\n",
        "      FEATURE_PROCESS_LOGGER.to_csv(PATH_FEATURE_PROCESS_LOGGER, index = True)\n",
        "\n",
        "\n",
        "    #find unprocessed files\n",
        "    unattempted            = FEATURE_PROCESS_LOGGER[FEATURE_PROCESS_LOGGER.FEATURE_PROCESSED == 'UNATTEMPTED']\n",
        "\n",
        "    #find records with ROI available\n",
        "    unattempted            = unattempted[unattempted.ROI_PROCESSED]\n",
        "\n",
        "    #get train and test ROI FILENAMES\n",
        "    train = unattempted[unattempted.sample_type == 'train'].ROI_FILENAME.unique().tolist()\n",
        "    test  = unattempted[unattempted.sample_type == 'test' ].ROI_FILENAME.unique().tolist()\n",
        "\n",
        "    #extract feature for train,test \n",
        "    if len(test) > 0:  \n",
        "      failed_test_samples,failed_roi_test_files   = extract_features( roi_files = test,\n",
        "                                                    sample_type      = 'test',  \n",
        "                                                    roi_path         = ROI_PATH, \n",
        "                                                    feature_path     = FEATURE_PATH,\n",
        "                                                    flatten_features = False, \n",
        "                                                    resize_to_shape  = RESIZE_SHAPE,\n",
        "                                                    batch_size       = TEST_STREAM_SIZE)\n",
        "      print(f\"#Omitted Test Files: {len(failed_roi_test_files)}\")\n",
        "    else:\n",
        "      print(\"Test files are already processed\")\n",
        "    sleep(1)\n",
        "    if len(train) > 0:\n",
        "      failed_train_samples,failed_roi_train_files = extract_features(roi_files = train,\n",
        "                                                    sample_type      = 'train',  \n",
        "                                                    roi_path         = ROI_PATH, \n",
        "                                                    feature_path     = FEATURE_PATH,\n",
        "                                                    flatten_features = False, \n",
        "                                                    resize_to_shape  = RESIZE_SHAPE,\n",
        "                                                    batch_size       = TRAIN_STREAM_SIZE)      \n",
        "      print(f\"Omitted Train Files: {len(failed_roi_train_files)}\")\n",
        "    else:\n",
        "      print(\"Train files are already processed\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG-P2gW075gk",
        "colab_type": "text"
      },
      "source": [
        "#MODEL TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqM88MGc7xYX",
        "colab_type": "text"
      },
      "source": [
        "##SETUP TENSORBOARD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05-MQiIj_M3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "#%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arHjjrsD_S42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "#!rm -rf ./logs/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgu18r1psO0s",
        "colab_type": "text"
      },
      "source": [
        "##MODEL ARCHITECTURE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubEwsL7c1P1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_OUT_SIZE():\n",
        "      ######TEST##########\n",
        "      sample_type  = 'test'\n",
        "      sub_path     = FEATURE_PATH + sample_type + \"/\"\n",
        "      options      = [f for f in listdir(sub_path) if isfile(join(sub_path, f))]\n",
        "      feature_read = h5py.File(sub_path + options[0],'r')\n",
        "      for key,value in feature_read.items():\n",
        "          value = value[:]\n",
        "          break\n",
        "      feature_read.close()\n",
        "      return np.product(value.shape[1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIvyXh9LQBRQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cf81ca6c-1741-4345-e008-d31e0772746c"
      },
      "source": [
        "if TRAIN_MODEL or PREPARE_SUBMISSION:\n",
        "    \t\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', \n",
        "                       verbose=1, patience=EARLY_STOP_PATIENCE,\n",
        "                       restore_best_weights      =RESTORE_BEST_WEIGHTS)\n",
        "\n",
        "    OUT_SIZE = -1\n",
        "\n",
        "    if CONTINUE_TRAINING or PREPARE_SUBMISSION:\n",
        "        models_available = [f for f in listdir(MODEL_SUB_FODLER) if f.startswith(MODEL_VERSION)]\n",
        "        models_keys      = [s.split('_')[2:] for s in models_available]\n",
        "        models_keys      = sorted(models_keys, key = lambda k: (int(k[0]),int(k[1]),int(k[2]))) \n",
        "        latest_model     = models_keys[-1]\n",
        "        model_name       = MODEL_VERSION + '_'.join(latest_model)\n",
        "        model            = models.load_model(MODEL_SUB_FODLER +model_name)\n",
        "        print(\"Successfully loaded model \", model_name)\n",
        "    else:\n",
        "        #os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' #turn off warnings  \n",
        "        tf.get_logger().setLevel('INFO')\n",
        "\n",
        "        model =models.Sequential()\n",
        "        b,w,c = RESIZE_SHAPE\n",
        "\n",
        "\n",
        "        model.add(layers.TimeDistributed(layers.Conv2D(8, (3, 3), activation='relu'), \n",
        "                input_shape=(MIN_FRAME_COUNT,b,w,c), name = 'TDCNN1'))\n",
        "        \n",
        "\n",
        "        model.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2), strides=(1, 1)), name = 'TDPOOL1'))\n",
        "\n",
        "        model.add(layers.TimeDistributed(layers.Conv2D(16, (4,4), activation='relu'), name = 'TDCNN2'))\n",
        "        model.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2)), name = 'TDPOOL2'))\n",
        "\n",
        "        model.add(layers.TimeDistributed(layers.Conv2D(32, (4,4), activation='relu'), name = 'TDCNN3'))\n",
        "        model.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2)), name = 'TDPOOL3'))\n",
        "\n",
        "        model.add(layers.TimeDistributed(layers.Conv2D(64, (4,4), activation='relu'), name = 'TDCNN4'))\n",
        "        model.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2)), name = 'TDPOOL4'))\n",
        "\n",
        "        # extract features and dropout \n",
        "        model.add(layers.TimeDistributed(Flatten()))\n",
        "        model.add(layers.Dropout(0.5))\n",
        "\n",
        "\n",
        "        model.add(LSTM(512, kernel_regularizer=L1L2(l1=LSTM_L1_REGULARIZATION,\n",
        "                                                    l2=LSTM_L2_REGULARIZATION),\n",
        "                            name = \"1_LSTM\", dropout=0.5))\n",
        "\n",
        "        model.add(layers.Dense(128, activation='relu',\n",
        "                              kernel_regularizer=L1L2(l1=L1_REGULARIZATION,\n",
        "                                                    l2=L2_REGULARIZATION),\n",
        "                              name = \"2_Dense\"))\n",
        "\n",
        "        model.add(layers.Dropout(0.5, name = \"3_Dropout\"))\n",
        "\n",
        "        model.add(layers.Dense(32, activation='relu',\n",
        "                              kernel_regularizer=L1L2(l1=L1_REGULARIZATION,\n",
        "                                                    l2=L2_REGULARIZATION),\n",
        "                              name = \"4_Dense\"))\n",
        "\n",
        "        model.add(layers.Dropout(0.5, name = \"5_Dropout\"))\n",
        "\n",
        "        model.add(layers.Dense(1, activation='sigmoid',\n",
        "                              kernel_regularizer=L1L2(l1=L1_REGULARIZATION,\n",
        "                                                    l2=L2_REGULARIZATION),\n",
        "                              name = \"6_Dense\"))\n",
        "\n",
        "        model.compile(optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "                                                    loss='binary_crossentropy',\n",
        "                                                    metrics=['acc'])\n",
        "        model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully loaded model  MODEL_V72_2_244_25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9W_4izOrSkZ",
        "colab_type": "text"
      },
      "source": [
        "##FUNCTION TO LOAD DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoGSK34AdIfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if TRAIN_MODEL:    \n",
        "\n",
        "    #load label dataframe      \n",
        "    DF_LABEL = pd.read_csv(LABELS_FILENAME)      \n",
        "    DF_LABEL.set_index('filename', inplace = True)\n",
        "    \n",
        "    sub_path_train = FEATURE_PATH + 'train' + \"/\"\n",
        "    train_options = [f for f in listdir(sub_path_train) if isfile(join(sub_path_train, f))]\n",
        "\n",
        "    sub_path_test = FEATURE_PATH + 'test' + \"/\"\n",
        "    test_options = [f for f in listdir(sub_path_test) if isfile(join(sub_path_test, f))]\n",
        "\n",
        "    feat_log = pd.read_csv(PATH_FEATURE_PROCESS_LOGGER, index_col = 0)\n",
        "    feat_log_test = feat_log[feat_log.sample_type == 'test']\n",
        "    gb_test = feat_log_test.groupby('FEATURE_FILENAME').agg({'label':'sum'})\n",
        "    gb_test = gb_test[gb_test.label > 1]\n",
        "    \n",
        "    \n",
        "    file_options = {}\n",
        "    file_options['train'] = train_options\n",
        "    file_options['test']  = [i for i in gb_test.index if i.startswith('test')]\n",
        "\n",
        "\n",
        "     \n",
        "    def load_data_folder(sample_type,n_samples,set_choice = None):\n",
        "\n",
        "          #find options available for sample type\n",
        "          sub_path  = FEATURE_PATH + sample_type + \"/\"\n",
        "          options   = file_options[sample_type]\n",
        "\n",
        "          #choose file to use\n",
        "          if set_choice is None:\n",
        "              use_file = choice(options)\n",
        "          else:\n",
        "              use_file = options[set_choice]\n",
        "\n",
        "          #open reader\n",
        "          feature_read = h5py.File(sub_path + use_file,'r')\n",
        "          \n",
        "          samples           = list(feature_read.keys())                          #all available files\n",
        "          samples           = [i for i in samples if i in train_metadata.index]  #all files that have label\n",
        "\n",
        "          min_ = min(n_samples,len(samples))                                     #downsample\n",
        "          samples = sample(samples,min_)                                         #sampled down files\n",
        "          samples_features = [feature_read[key][:] for key in samples]           #read features\n",
        "          print(f\"{use_file} features loaded\")\n",
        "\n",
        "          #close reader\n",
        "          feature_read.close()\n",
        "               \n",
        "\n",
        "          sample_labels    = DF_LABEL.loc[samples,'stalled'].to_numpy()\n",
        "          print(f\"{use_file} labels   loaded\") \n",
        "\n",
        "          samples_features = np.array(samples_features)\n",
        "\n",
        "          return samples_features,sample_labels,use_file         \n",
        "      \n",
        "\n",
        "    def load_data(set_num, train_prop = TRAIN_DOWN_SIZE,test_prop = VALIDATION_SIZE):\n",
        "\n",
        "      ######TEST##########      \n",
        "      test_features,test_labels,test_file = load_data_folder(sample_type = \"test\",\n",
        "                                                             n_samples   = VALIDATION_SIZE,\n",
        "                                                             set_choice = None)\n",
        "\n",
        "      ######TRAIN##########\n",
        "      train_features, train_labels,train_file = load_data_folder(sample_type = \"train\",\n",
        "                                                                 n_samples   = TRAIN_DOWN_SIZE,\n",
        "                                                                 set_choice = set_num)\n",
        "\n",
        "      return train_features, train_labels, test_features, test_labels, train_file, test_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0ieAVserDIU",
        "colab_type": "text"
      },
      "source": [
        "##FUNCTIONS FOR FITTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg26yxpn_ZUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlhyKCRJvnP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_tuple(d, path, save = False, ylim_ = (-1,-1)):\n",
        "  import matplotlib.pyplot as plt\n",
        "  for (k, v) in d:\n",
        "      if k not in ['test_mcc', 'train_mcc']:\n",
        "       k = k if k[0] == 'v' else 'train_' + k\n",
        "      plt.plot(range(1, len(v) + 1), v, '.-', label=k)\n",
        "      # NOTE: changed `range(1, 4)` to mach actual values count\n",
        "  plt.legend()  # To draw legend\n",
        "  if ylim_[1]>0:\n",
        "    plt.ylim(ylim_[0],ylim_[1])\n",
        "  if save:\n",
        "    plt.savefig(path)\n",
        "  plt.show()\n",
        "  del plt\n",
        "\n",
        "def takespread(sequence, num):    \n",
        "    sequence = list(range(sequence))\n",
        "    length   = float(len(sequence))\n",
        "    return np.array([sequence[int(ceil(i * length / num))] for i in range(num)])\n",
        "\n",
        "def get_pred(features):\n",
        "    #features,labels = process_batch(features,labels,type_)\n",
        "    pred = model.predict(features)\n",
        "    pred = [item for sublist in pred for item in sublist]\n",
        "    pred = [1 if i>.5 else 0 for i in pred]  \n",
        "    return pred\n",
        "\n",
        "def get_mcc(pred,labels):\n",
        "    try:\n",
        "      return np.round(matthews_corrcoef(labels, pred),2)   \n",
        "    except:\n",
        "      return -2\n",
        "\n",
        "def plot_output_hist(train_features,test_features,path, \n",
        "                     save = False, FIT_1_SAMPLE = FIT_1_SAMPLE):\n",
        "    import matplotlib.pyplot as plt\n",
        "    bins = np.linspace(0, 1, 20)\n",
        "    \n",
        "    prediction =  model.predict(train_features)                   \n",
        "    pred_train = [item for sublist in prediction for item in sublist]\n",
        "    \n",
        "    prediction =  model.predict(test_features) \n",
        "    pred_test = [item for sublist in prediction for item in sublist]\n",
        "\n",
        "    plt.hist(pred_train, bins, alpha=0.5, label='train', density=True, color = \"skyblue\", ec=\"skyblue\")\n",
        "    plt.hist(pred_test, bins, alpha=0.5, label='test', density=True,color = \"red\", ec=\"red\")\n",
        "\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title(\"Output Distribution\")\n",
        "    if save:\n",
        "       plt.savefig(path)\n",
        "    plt.show()\n",
        "    del plt\n",
        "\n",
        "def test_on_n_sets(n = 10):\n",
        "  predictions = []\n",
        "  labels      = []\n",
        "  for i in range(n):\n",
        "      print(\"Set \",i+1)\n",
        "      samples_features,sample_labels,use_file  = load_data_folder('test',TEST_STREAM_SIZE,set_choice = None)      \n",
        "      predictions += get_pred(samples_features)\n",
        "      labels      += list(sample_labels)\n",
        "  \n",
        "  print(f\"Compiled test set details: Total Samples  {len(labels)}   Total Positive Sample {sum(labels)}\")\n",
        "  print(f\"MCC computed on {n} sets: \", get_mcc(predictions,labels))\n",
        "\n",
        "  (tn, fp, fn, tp) = confusion_matrix(labels, predictions).ravel()\n",
        "  recall    = tp/(tp+fn)\n",
        "  precision = tp/(tp+fp)\n",
        "  \n",
        "  print(f\"Precision: {precision}     Recall {recall}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayjX0t--quS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maxper(validation_labels, pred):\n",
        "    # Save image in set directory \n",
        "    l = []\n",
        "    max_ = -1\n",
        "    best = -1\n",
        "    for thres in range(1,100):\n",
        "        pred = np.array(pred).flatten()\n",
        "        pred_ = [1 if i> thres/100 else 0 for i in pred]\n",
        "        try:\n",
        "            m = matthews_corrcoef(validation_labels, pred_)\n",
        "            if m> max_ :\n",
        "              max_ = m\n",
        "              best = thres\n",
        "            l.append(m)\n",
        "        except:\n",
        "            pass\n",
        "    print(\"MCC: \" , np.round(max(l),2))\n",
        "    return np.round(max(l),2),best,l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcqFn2usfDAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if False: #only for testing\n",
        "\n",
        "  for i in range(len(train_options)):\n",
        "      train_features, train_labels, test_features, test_labels,_,_,_,_ = load_data(train_options[i],\n",
        "                                                                      .7)\n",
        "      print(\"\\n\\nSet: \", i)\n",
        "      for t in ['last_n','random', 'even_spaced','random_subsection']:\n",
        "        test_mcc  = get_mcc(test_features,test_labels,t) #random, even_spaced\n",
        "        train_mcc = get_mcc(train_features,train_labels,t)\n",
        "        print(t,\"\\t\\ttest:\",test_mcc,\"\\ttrain:\",train_mcc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEGaikqBH0qG",
        "colab_type": "text"
      },
      "source": [
        "##TRAINING SETUP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmE1ZrBJHlho",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "087875a2-81fb-4eea-98fb-a96acba44a36"
      },
      "source": [
        "if TRAIN_MODEL:\n",
        "\n",
        "  n_sets = len(train_options)\n",
        "  test_mcc_list  = []\n",
        "  train_mcc_list = []\n",
        "  train_features, train_labels, test_features,test_labels, _, _ = load_data(0, 1,  1)\n",
        "\n",
        "  SAMPLES_IN_TRAIN_STREAM = train_features.shape[0]\n",
        "\n",
        "  hyper_param = pd.DataFrame({\n",
        "      'EPOCH_PER_SET'         : EPOCH_PER_SET,\n",
        "      'GLOBAL_ITER'           : GLOBAL_ITER,\n",
        "      'FRAME_SUBSET_TYPE'     : FRAME_SUBSET_TYPE,\n",
        "      'MIN_FRAME_COUNT'       : MIN_FRAME_COUNT,\n",
        "      'LSTM_L1_REGULARIZATION': LSTM_L1_REGULARIZATION,\n",
        "      'LSTM_L2_REGULARIZATION': LSTM_L2_REGULARIZATION,\n",
        "      'L1_REGULARIZATION'     : L1_REGULARIZATION,\n",
        "      'L2_REGULARIZATION'     : L2_REGULARIZATION,\n",
        "      'TRAIN_DOWN_SIZE'       : TRAIN_DOWN_SIZE,\n",
        "      'MINI_BATCH_SIZE'       : MINI_BATCH_SIZE,\n",
        "      'CLASS_WTS'             : str(CLASS_WTS),\n",
        "      'FEATURE_USED'          : FEATURE_USED,\n",
        "      'ROI_ROT'               : ROI_ROT,\n",
        "      'PREPROCESS'            : PREPROCESS,\n",
        "      'LEARNING_RATE'         : LEARNING_RATE,\n",
        "      'WEIGHT_BY_FRAME'       : WEIGHT_BY_FRAME,\n",
        "      'DEPTH_OF_BASE'         : DEPTH_OF_BASE,\n",
        "      'RESIZE_SHAPE'          : str(RESIZE_SHAPE),\n",
        "      'TEST_STREAM_SIZE'      : int(test_features.shape[0]/VALIDATION_SIZE),\n",
        "      'TRAIN_STREAM_SIZE'     : train_features.shape[0],\n",
        "      'OUT_SIZE'              : OUT_SIZE,\n",
        "      'FIT_1_SAMPLE'          : FIT_1_SAMPLE,\n",
        "      'MINI_BATCH_ITERATION'  : MINI_BATCH_ITERATION\n",
        "                              },\\\n",
        "                              index = ['Value'])\n",
        "  hyper_param.T.to_csv(HYP_PARAM_FILE)\n",
        "\n",
        "  try:  \n",
        "    HISTORY = pd.read_csv(PERF_FILE_FORMAT + model_name + '.csv')\n",
        "    HISTORY = HISTORY.to_dict()\n",
        "    for key,value in HISTORY.items():\n",
        "        HISTORY[key] = list(value.values())\n",
        "    \n",
        "\n",
        "    print(\"Loaded performance history of \", model_name)\n",
        "  except:\n",
        "    HISTORY = {'loss':[], 'acc':[], 'val_loss':[], 'val_acc':[]}\n",
        "  \n",
        "  try:\n",
        "    iters_completed = int(model_name.split(\"_\")[-3]) \n",
        "    sets_completed  = int(model_name.split(\"_\")[-2]) + 1\n",
        "    print(\"Already completed-  Iterations: {} Set_Number: {}\".format(iters_completed,\n",
        "                                                               sets_completed))\n",
        "  except:\n",
        "    iters_completed = 0\n",
        "    sets_completed  = 0\n",
        "\n",
        "  do_once_ = True\n",
        "  fname_list = []\n",
        "  del train_features, train_labels, test_features,test_labels\n",
        "\n",
        "  try:\n",
        "    CONFUSION_MAT = pd.read_csv(CONFUSION_MAT_FILE, index_col = 0)\n",
        "    print(\"Confusion Matrix loaded\")\n",
        "  except:\n",
        "    CONFUSION_MAT = pd.DataFrame({},\n",
        "                                  columns = ['TN', 'FP', 'FN', 'TP', 'Precision', \n",
        "                                            'Recall', 'test_file', 'fname'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test76.h5 features loaded\n",
            "test76.h5 labels   loaded\n",
            "train1.h5 features loaded\n",
            "train1.h5 labels   loaded\n",
            "Loaded performance history of  MODEL_V72_2_244_25\n",
            "Already completed-  Iterations: 2 Set_Number: 245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uY8gHs-H9tl",
        "colab_type": "text"
      },
      "source": [
        "##FITTING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnBY1hpYZyWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#manual intervention required. run function to turn off CNN training\n",
        "def toggle_CNN_tuning():\n",
        "  for ind,layer in enumerate(model.layers[:9]):    \n",
        "    model.layers[ind].trainable = not model.layers[ind].trainable\n",
        "    print(ind, \"\\t\" , model.layers[ind].name, \"\\t\" , model.layers[ind].trainable, \"\\t\", model.layers[ind])\n",
        "  #model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L8hI2ATgtwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if TRAIN_MODEL:\n",
        "  \n",
        "  for iteration in range(iters_completed,GLOBAL_ITER):\n",
        "    if do_once_:\n",
        "      pass\n",
        "    else:\n",
        "      sets_completed = 0\n",
        "    do_once_ = False\n",
        "    failed_iterations = []\n",
        "    for set_num_, choice_train_set in enumerate(train_options[sets_completed:]):\n",
        "          if set_num_ > 150:\n",
        "            LEARNING_RATE = LEARNING_RATE/2\n",
        "          try:\n",
        "\n",
        "              set_num = set_num_ + sets_completed\n",
        "              if set_num % 50 == 0:\n",
        "                toggle_CNN_tuning()\n",
        "              choice_train_set = train_options[set_num]\n",
        "              do_once = False\n",
        "\n",
        "              \n",
        "              train_features, train_labels,test_features,test_labels,train_file, test_file = load_data(set_num)               \n",
        "              assert choice_train_set == train_file\n",
        "\n",
        "              print(\"\\n\\n\\nGlobal Iteration: ({}/{}) Set Number: ({}/{}) Files: ({},{}) Version: {}\".format(iteration,\n",
        "                                                        GLOBAL_ITER,\n",
        "                                                        set_num,\n",
        "                                                        n_sets,\n",
        "                                                        train_file,\n",
        "                                                        test_file,\n",
        "                                                        MODEL_VERSION[:-1]) )\n",
        "\n",
        "\n",
        "\n",
        "              if CLASS_WTS == 'auto':\n",
        "\n",
        "                #get auto train weights\n",
        "                train_positive_prop = np.mean(train_labels)\n",
        "                wt_dict_train = {0 : 100*train_positive_prop,\n",
        "                                 1 : 100*(1-train_positive_prop)}\n",
        "                train_wts   = np.array([wt_dict_train[i] for i in train_labels])\n",
        "\n",
        "                #get auto test weights\n",
        "                test_positive_prop  = np.mean(test_labels)                \n",
        "                wt_dict_test  = {0 : 100*test_positive_prop,\n",
        "                                 1 : 100*(1-test_positive_prop)}\n",
        "                test_wts    = np.array([wt_dict_test[i]  for i in test_labels])\n",
        "\n",
        "\n",
        "              else:\n",
        "                assert type(CLASS_WTS) == dict\n",
        "                \n",
        "                #GET WEIGHTS\n",
        "                test_wts    = np.array([CLASS_WTS[i] for i in test_labels])\n",
        "                train_wts   = np.array([CLASS_WTS[i] for i in train_labels])\n",
        "\n",
        "\n",
        "              #PRINT PROPORTION OF POSITIVE SAMPLES AND AVERAGE WEIGHTS\n",
        "              print(\"Avg Train Wt: {} Avg Test Wt: {}  Train Prop: {}  Test Prop: {}\".format(\\\n",
        "                        np.round(np.mean(train_wts),2), np.round(np.mean(test_wts),2),\n",
        "                        np.round(np.mean(train_labels),2), np.round(np.mean(test_labels),2)))\n",
        "               \n",
        "              #FITTING MODEL\n",
        "              history = model.fit(train_features,\n",
        "                                  train_labels,\n",
        "                                  batch_size       = MINI_BATCH_SIZE, \n",
        "                                  epochs           = EPOCH_PER_SET, \n",
        "                                  sample_weight    = train_wts,      \n",
        "                                  validation_data  = (test_features, \n",
        "                                                      test_labels,\n",
        "                                                      test_wts),\n",
        "                                  callbacks=[es])#,\n",
        "                                  #callbacks=[tensorboard_callback])\n",
        "              history = history.history\n",
        "              \n",
        "              #filename (used for plots and model)\n",
        "              fname =  \"{}_{}_{}\".format(iteration,\n",
        "                                                        set_num,\n",
        "                                                        EPOCH_PER_SET)\n",
        "              fname_list.append(fname)\n",
        "\n",
        "\n",
        "              for key in history.keys():\n",
        "                  HISTORY[key] += history[key]        \n",
        "\n",
        "\n",
        "              #plot and save accuracy          \n",
        "              #plot_tuple(tuple((k, HISTORY[k]) for k in ('acc', 'val_acc')),\\\n",
        "              #          path= ACC_IMAGE_NAME.format(fname),\\\n",
        "              #          save = True)\n",
        "              \n",
        "              #plot and save loss - train,test\n",
        "              ylim_ =  (np.min(HISTORY['loss'][-20:]), np.max(HISTORY['loss'][-20:]))\n",
        "              plot_tuple(tuple((k, HISTORY[k][-50:]) for k in ('loss', 'val_loss')),\\\n",
        "                            path= LOSS_IMAGE_NAME.format(fname),\\\n",
        "                            save = True)#,\n",
        "                            #ylim_ = [int(ylim_[1]-1),int(ylim_[1]+2)])\n",
        "\n",
        "                          \n",
        "              #save model\n",
        "              model.save(MODEL_FILENAME + fname)\n",
        "              print(\"Model saved to : \", MODEL_FILENAME + fname)\n",
        "              model_name = MODEL_VERSION + fname\n",
        "              perf_df = pd.DataFrame(HISTORY)\n",
        "              perf_df.to_csv(MODEL_PERF_FILE.format(fname), index = False)   \n",
        "\n",
        "              #if set_num%5 == 0 or set_num == len(train_options) -1:\n",
        "              if True:\n",
        "                  #get predictions\n",
        "                  test_pred = get_pred(test_features)\n",
        "                  train_pred= get_pred(train_features)\n",
        "\n",
        "                  #confusion matrix calculations                  \n",
        "                  cm = confusion_matrix(test_labels, test_pred).ravel()\n",
        "\n",
        "                  if cm.shape[0] > 1:\n",
        "                    (tn, fp, fn, tp) = cm\n",
        "                    recall    = tp/(tp+fn)\n",
        "                    precision = tp/(tp+fp)\n",
        "                  else:\n",
        "                    tn = cm[0]\n",
        "                    recall    = float('nan')\n",
        "                    precision = float('nan')\n",
        "                    (fp, fn, tp) = [float('nan')]*3\n",
        "\n",
        "                  \n",
        "                  if False:\n",
        "                    \n",
        "                    test2_f,test2_l,test2_n = load_data_folder('test', TEST_STREAM_SIZE)\n",
        "                    print(f\"Evaluating extra test set {test2_n}\")\n",
        "\n",
        "                    test2_pred = get_pred(test2_f)\n",
        "                    cm2 = confusion_matrix(test2_l, test2_pred).ravel()\n",
        "\n",
        "                    if cm.shape[0] > 1:                      \n",
        "                      (tn2, fp2, fn2, tp2) = cm2                      \n",
        "                      tn += tn2\n",
        "                      fp += fp2\n",
        "                      fn += fn2\n",
        "                      tp += tp2\n",
        "\n",
        "                    else:\n",
        "                      tn = cm[0]\n",
        "\n",
        "\n",
        "                    try:\n",
        "                      recall    = tp/(tp+fn)\n",
        "                      precision = tp/(tp+fp)\n",
        "                    except:\n",
        "                      recall    = float('nan')\n",
        "                      precision = float('nan')                    \n",
        "\n",
        "                    test_file += \" + \" + test2_n\n",
        "                    \n",
        "\n",
        "                    #to be picked up for mcc calculations\n",
        "                    test_pred   = list(test_pred)   + list(test2_pred)\n",
        "                    test_labels = list(test_labels) + list(test_labels)\n",
        "\n",
        "                  CONFUSION_MAT = CONFUSION_MAT.append({'TN' : tn, 'FP' : fp, \n",
        "                                                        'FN' : fn, 'TP': tp, \n",
        "                                                       'Precision':round(precision,2), \n",
        "                                                       'Recall':round(recall,2),\n",
        "                                                       'test_file': test_file, \n",
        "                                                       'fname': fname},\n",
        "                                                       ignore_index = True)\n",
        "                  CONFUSION_MAT.to_csv(CONFUSION_MAT_PATH, index = True)\n",
        "                  \n",
        "\n",
        "                  #mcc calculations\n",
        "                  test_mcc  = get_mcc(test_pred,test_labels)\n",
        "                  train_mcc = get_mcc(train_pred,train_labels)\n",
        "\n",
        "\n",
        "\n",
        "                  #append values to lists\n",
        "                  test_mcc_list.append(test_mcc)\n",
        "                  train_mcc_list.append(train_mcc)\n",
        "                  check = len(fname_list) == len(train_mcc_list)\n",
        "                  MCC_df = pd.DataFrame({'Test MCC':test_mcc_list,'Train MCC':train_mcc_list},\n",
        "                                        index = fname_list if check else None)\n",
        "                  MCC_df.to_csv(MODEL_MCC_FORMAT.format(fname), index = check)\n",
        "\n",
        "                  #plot graphs\n",
        "                  plot_tuple((('train_mcc',train_mcc_list),('test_mcc', test_mcc_list)),\n",
        "                            path= MCC_IMAGE_NAME.format(fname),\n",
        "                            save = True) \n",
        "                  \n",
        "                  #plot output histogram\n",
        "                  plot_output_hist(train_features,test_features,\n",
        "                            path= OUT_DIST_IMAGE_NAME.format(fname),\n",
        "                            save = True)\n",
        "\n",
        "                  if test_mcc > BIGGER_SAMPLE_TEST_THRES:\n",
        "                    del train_features,test_features\n",
        "                    test_on_n_sets(n = int(10*test_mcc*3))\n",
        "          except:\n",
        "            failed_iterations.append((set_num_, choice_train_set, sys.exc_info()[0]))\n",
        "            failed_iterations_df = pd.DataFrame(failed_iterations, \n",
        "                                                columns =['SET_NUM', 'CHOICE_TRAIN_SET', 'ERROR_NAME']) \n",
        "            failed_iterations_df.to_csv(PATH_TRAIN_ERRORS, index = False)\n",
        "\n",
        "            print(\"Failed: \",failed_iterations[-1] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqyBXtOfwdBJ",
        "colab_type": "text"
      },
      "source": [
        "# PREPARE SUBMISSION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAjEc_IE4MM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PREPARE_SUBMISSION = True\n",
        "if PREPARE_SUBMISSION:\n",
        "  SUBMISSION_DOWNLOAD_VIDEOS = False\n",
        "  SUB_EXTRACT_ROI            = False\n",
        "  SUB_EXTRACT_FEATURES       = True\n",
        "  SUB_PROCESS_CHUNK_SIZE     = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siyvISz7lQA-",
        "colab_type": "text"
      },
      "source": [
        "## DECLARE MODEL VERSIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cssy_Oib4D5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH_MODEL_VERSION = MODEL_SUB_FODLER + model_name\n",
        "model            = models.load_model(PATH_MODEL_VERSION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvC60ajFnmXn",
        "colab_type": "text"
      },
      "source": [
        "## PATH VARIABLES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jou8TpmLtnWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import mkdir\n",
        "from os.path import isdir\n",
        "\n",
        "#define paths\n",
        "SUB_PATH_ROOT = PATH_PROJ + \"SUBMISSION/\"\n",
        "SUB_VIDEO_FILES_PATH =  SUB_PATH_ROOT + \"VIDEO_FOLDER/\"                    #PATH TO VIDEO FILES IN GDRIVE\n",
        "\n",
        "\n",
        "#ROI\n",
        "roi_rot  = (\"ROI_rotated/\"  if ROI_ROT else \"ROI/\") \n",
        "SUB_ROI_PATH = SUB_PATH_ROOT + \"ROI_FOLDER/\" + roi_rot                #PATH TO ROI DATA\n",
        "if not isdir(SUB_ROI_PATH):\n",
        "  mkdir(SUB_ROI_PATH)\n",
        "  mkdir(SUB_ROI_PATH+\"test/\")\n",
        "  print(\"Folder\" , SUB_ROI_PATH , \"and subfolders created\")\n",
        "PATH_SUB_ROI_PROCESS_LOGGER = SUB_ROI_PATH  + 'ROI_PROCESS_LOGGER.csv'\n",
        "\n",
        "\n",
        "#FEATURES\n",
        "SUB_FEATURE_USED = \"TDCNN_features_depth_{}_{}_{}/\".format(DEPTH_OF_BASE,roi_rot[:-1],RESIZE_SHAPE[0])\n",
        "if PREPROCESS:\n",
        "   SUB_FEATURE_USED = SUB_FEATURE_USED[:-1] + \"_P/\"\n",
        "   \n",
        "SUB_FEATURE_PATH = SUB_PATH_ROOT  + \"FEATURE_FOLDER/\" + SUB_FEATURE_USED #PATH TO FEATURES EXTRACTED FROM ROI\n",
        "#FEATURE_PATH = PATH_ROOT + \"FEATURE_FOLDER/\" + 'extracted_features_full_depth/'\n",
        "if not isdir(SUB_FEATURE_PATH):\n",
        "  mkdir(SUB_FEATURE_PATH)\n",
        "  print(\"Folder\" , SUB_FEATURE_PATH , \"created\")  \n",
        "  mkdir(SUB_FEATURE_PATH + '/test/')\n",
        "PATH_SUB_FEATURE_PROCESS_LOGGER = SUB_FEATURE_PATH  + 'FEATURE_PROCESS_LOGGER.csv'\n",
        "\n",
        "#filenames\n",
        "SUBMISSION_FORMAT_PATH = PATH_PROJ + \"submission_format.csv\"\n",
        "TEST_PREDICTIONS_PATH  = SUB_PATH_ROOT + \"TEST_PREDICTIONS_{}.csv\".format(MODEL_VERSION)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0Mds8nnmTvd",
        "colab_type": "text"
      },
      "source": [
        "## DOWNLOAD SUBMISSION VIDEOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfBr3LIXmWNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if SUBMISSION_DOWNLOAD_VIDEOS:\n",
        "  #import library to interact with s3\n",
        "  import boto3\n",
        "\n",
        "  #setup s3 access\n",
        "  s3r = boto3.resource('s3',\n",
        "                      aws_access_key_id='AKIAR3X7R6S3PM6G56GS',\n",
        "                      aws_secret_access_key= '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')\n",
        "  buck = s3r.Bucket('drivendata-competition-clog-loss')\n",
        "\n",
        "  failed = []\n",
        "\n",
        "  for my_bucket_object in tqdm(buck.objects.all(), position=0, leave=True):\n",
        "      try:\n",
        "        is_train, filename = my_bucket_object.key.split(\"/\")\n",
        "        is_train = is_train == 'train'\n",
        "        if is_train:      \n",
        "                break\n",
        "\n",
        "        buck.download_file(my_bucket_object.key, SUB_VIDEO_FILES_PATH + filename)  \n",
        "      except:\n",
        "        failed.append(f)\n",
        "        print(\".\", sep = \"\", end = \"\")\n",
        "  if len(failed) > 0:\n",
        "    print(str(len(failed)) + \" files failed to load.\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDxN4GQMm5Zn",
        "colab_type": "text"
      },
      "source": [
        "## EXTRACT ROI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8OQqxOHm8KL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if SUB_EXTRACT_ROI :\n",
        " \n",
        "    #mark all undecided \n",
        "    TEST_TRAIN_SPLIT                = pd.read_csv(SUBMISSION_FORMAT_PATH, index_col = 0)\n",
        "    TEST_TRAIN_SPLIT['sample_type'] = 'test'\n",
        "\n",
        "    #ROI PROCESS LOGGER\n",
        "    TEST_TRAIN_SPLIT['ROI_PROCESSED'] = False\n",
        "    TEST_TRAIN_SPLIT['ROI_FILENAME'] = 'NOT_AVAILABLE'\n",
        "    TEST_TRAIN_SPLIT.to_csv(PATH_SUB_ROI_PROCESS_LOGGER, index = True)\n",
        "    print(\"Saving submission roi process logger\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYToT-0qog6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if SUB_EXTRACT_ROI:\n",
        "\n",
        "    #read roi log file and remove already processed files\n",
        "    logger        = pd.read_csv(PATH_SUB_ROI_PROCESS_LOGGER, index_col= 0)\n",
        "    logger        = logger[~logger.ROI_PROCESSED]\n",
        "\n",
        "    #files yet to be processed\n",
        "    test          = logger[logger.sample_type == 'test'].index.tolist()         \n",
        " \n",
        "    if len(test) > 0:\n",
        "      #extract roi and save for train and test   \n",
        "      extract_roi_and_store_tensors(filenames_list = test,  \n",
        "                                      sample_type = 'test', \n",
        "                                      roi_path     = SUB_ROI_PATH, \n",
        "                                      video_path   = SUB_VIDEO_FILES_PATH,\n",
        "                                      batch_size   = SUB_PROCESS_CHUNK_SIZE,\n",
        "                                      submission   = True)\n",
        "    else:\n",
        "      print(\"Test file already ROI processed\")\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OJTSUVUm8pj",
        "colab_type": "text"
      },
      "source": [
        "## EXTRACT FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKzuwL04nAup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if SUB_EXTRACT_FEATURES:\n",
        "\n",
        "    #READ FEATURE LOGGER\n",
        "    try:\n",
        "      FEATURE_PROCESS_LOGGER = pd.read_csv(PATH_SUB_FEATURE_PROCESS_LOGGER, \n",
        "                                           index_col = 0)\n",
        "      \n",
        "      #refresh ROI_PROCESSED INFORMATION\n",
        "      roi_log         = pd.read_csv(PATH_SUB_ROI_PROCESS_LOGGER, index_col= 0)\n",
        "      cols_to_refresh = ['ROI_PROCESSED','ROI_FILENAME']\n",
        "      FEATURE_PROCESS_LOGGER.drop(columns=cols_to_refresh, inplace = True)\n",
        "      FEATURE_PROCESS_LOGGER = pd.merge(FEATURE_PROCESS_LOGGER,roi_log[cols_to_refresh],\n",
        "                                        on = 'filename', \n",
        "                                        validate = 'one_to_one')\n",
        "\n",
        "\n",
        "      print(\"Loaded FEATURE_PROCESS_LOGGER\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "      print(\"No SUB_FEATURE_PROCESS_LOGGER available. Creating new one..\")\n",
        "\n",
        "      #create feature logger from ROI logger\n",
        "      FEATURE_PROCESS_LOGGER                      = pd.read_csv(PATH_SUB_ROI_PROCESS_LOGGER, index_col= 0)\n",
        "\n",
        "      FEATURE_PROCESS_LOGGER['FEATURE_PROCESSED'] = 'UNATTEMPTED'  #False refers to unattempted files\n",
        "      FEATURE_PROCESS_LOGGER['FEATURE_FILENAME']  = 'NOT_AVAILABLE'\n",
        "\n",
        "      #save to disk\n",
        "      FEATURE_PROCESS_LOGGER.to_csv(PATH_SUB_FEATURE_PROCESS_LOGGER, index = True)\n",
        "\n",
        "\n",
        "    #find unprocessed files\n",
        "    unattempted            = FEATURE_PROCESS_LOGGER[FEATURE_PROCESS_LOGGER.FEATURE_PROCESSED == 'UNATTEMPTED']\n",
        "\n",
        "    #find records with ROI available\n",
        "    unattempted            = unattempted[unattempted.ROI_PROCESSED]\n",
        "\n",
        "    #get train and test ROI FILENAMES\n",
        "    test  = unattempted[unattempted.sample_type == 'test' ].ROI_FILENAME.unique().tolist()\n",
        "\n",
        "    #extract feature for train,test \n",
        "    if len(test) > 0:  \n",
        "      failed_test_samples,failed_roi_test_files   = extract_features( roi_files = test,\n",
        "                                                    sample_type      = 'test',  \n",
        "                                                    roi_path         = SUB_ROI_PATH, \n",
        "                                                    feature_path     = SUB_FEATURE_PATH,\n",
        "                                                    flatten_features = False, \n",
        "                                                    resize_to_shape  = RESIZE_SHAPE,\n",
        "                                                    batch_size       = SUB_PROCESS_CHUNK_SIZE,\n",
        "                                                    submission       = True)\n",
        "      print(f\"#Omitted Test Files: {len(failed_roi_test_files)}\")\n",
        "    else:\n",
        "      print(\"Test files are already processed\")\n",
        "    sleep(1)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwcfyk9Mm0jn",
        "colab_type": "text"
      },
      "source": [
        "## RUN MODEL ON SUBMISSION FILES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDoFlNXcmy6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_type = 'test'\n",
        "batch_size = MINI_BATCH_SIZE\n",
        "input_folder_path = FEATURE_PATH + sample_type + \"/\"\n",
        "\n",
        "if MAKE_PREDICTIONS:\n",
        "  df_sub = pd.read_csv(SUBMISSION_FORMAT_PATH)\n",
        "  df_sub.set_index(['filename'], inplace = True)\n",
        "  df_sub.to_csv(TEST_PREDICTIONS_PATH, index= True)\n",
        "  feature_files = [f for f in listdir(input_folder_path) if isfile(join(input_folder_path, f))]\n",
        "\n",
        "  all_files = set(list(df_sub.index))\n",
        "  thres = .5\n",
        "  ommited_files = []\n",
        "  l = []\n",
        "  n_ = len(feature_files)\n",
        "  for ff in feature_files:\n",
        "    \n",
        "    try:\n",
        "        print(f\"Processing file {ff[:-3]}/{n_} \")\n",
        "        sleep(1)\n",
        "\n",
        "        #feature data file stream\n",
        "        feature_read = h5py.File(input_folder_path + ff,'r')  \n",
        "\n",
        "        #read keys and features\n",
        "        f_keys = np.array([i for i in feature_read.keys()])\n",
        "        l += list(f_keys)\n",
        "        test_features = np.array([i[:] for i in feature_read.values()])\n",
        "\n",
        "        #prepare index for batch processing\n",
        "        n_keys = f_keys.shape[0]\n",
        "        index_list = list(range(0,n_keys,batch_size))\n",
        "        index_list = [ list(range(index_list[ind],index_list[ind+1])) for ind in range(len(index_list)-1)]\n",
        "        index_list += [list(range(index_list[-1][-1], n_keys))]\n",
        "\n",
        "        for ind in tqdm(index_list):    \n",
        "\n",
        "              #subset by index\n",
        "              x_samples,_   = process_batch(test_features[np.array(ind)],\n",
        "                                          [], type_ = FRAME_SUBSET_TYPE)\n",
        "\n",
        "              pred = model.predict(x_samples).flatten()\n",
        "              pred = np.array(pred > .5, dtype = int)\n",
        "              #print(np.mean(pred))\n",
        "              df_sub.loc[f_keys[ind],'stalled'] = pred\n",
        "\n",
        "        sleep(1)\n",
        "        print(ff, \" completed.\\n\\n\")\n",
        "        feature_read.close()\n",
        "        df_sub.to_csv(TEST_PREDICTIONS_PATH, index= True)\n",
        "        if len(set.difference(all_files,set(l))) == 0:\n",
        "          print(\"Process completed\")\n",
        "    except:\n",
        "      ommited_files.append(ff)\n",
        "      print(ff, \" ommited.\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
